{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GetCsvFromJsonOnline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrCHXnCAUR/u+kSOeXb1b2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjunjin/PFE-ING3-IA/blob/branch1/GetCsvFromJsonOnline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6pckrKUyomm"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4jVrGopjlXl"
      },
      "source": [
        "import urllib.request, json \n",
        "with urllib.request.urlopen(\"https://www.lemonde.fr/webservice/decodex/updates\") as url: # Obtenir le fichier JSON de https://www.lemonde.fr/webservice/decodex/updates\n",
        "    data = json.loads(url.read().decode())\n",
        "\n",
        "# data.keys() #On obtient le premier dictionnaire, sites et url, contient des dictionnaire\n",
        "# data[\"sites\"].keys() #On obtient le sous-dictionnaire, chaque site, contient des tableaux"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-DInfYIqUsj"
      },
      "source": [
        "# print(data)\n",
        "# data.keys() # Vérifie la forme que prend les fichier JSON\n",
        "# data[\"sites\"]\n",
        "# data[\"sites\"]['1980']\n",
        "# data[\"sites\"]['1980'][2]\n",
        "# data[\"sites\"].keys()\n",
        "# for i in data[\"sites\"].keys():\n",
        "#   print(type(i))\n",
        "\n",
        "# for i in data[\"sites\"].keys():\n",
        "#   print(data[\"sites\"][i][2])\n",
        "\n",
        "# data[\"urls\"].keys()\n",
        "# data[\"urls\"]['the-postillon.com']"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zbRRRijqmER"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for i in data[\"sites\"].keys(): # On créé un tableau avec la liste de tous les sites du fichier JSON. On a leur identifiant, leur niveau de fiabilité, une description, et 2 noms dont l'un est simplifié\n",
        "  # dataset.append([int(i),data[\"sites\"][i][0], data[\"sites\"][i][1], data[\"sites\"][i][2], data[\"sites\"][i][3]])\n",
        "  dataset.append([int(i),data[\"sites\"][i][0], data[\"sites\"][i][1]])\n",
        "\n",
        "# tableauSites = pd.DataFrame(dataset, columns=['Identifiant','Fiabilité','Description','Nom1','Nom2']) #Pour que le tableau prenne la forme d'un tableau (dataset n'étant en réalité qu'une liste)\n",
        "tableauSites = pd.DataFrame(dataset, columns=['Identifiant','Fiabilité','Description'])\n",
        "tableauSites"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-Z_CqyGkbHk"
      },
      "source": [
        "import re\n",
        "\n",
        "dt = [] #Pour déterminer toutes les lignes dans le JSON qui ne sont pas des liens twitter\n",
        "compteur = 0 #On note le numéro de chaque ligne qui n'est pas un lien twitter\n",
        "for i in data[\"urls\"].keys():\n",
        "  if not(re.search('twitter', i)):\n",
        "    dt.append(compteur)\n",
        "  compteur = compteur+1\n",
        "\n",
        "#On met tous les liens dans df avec des tuples (identifiant, lien)\n",
        "df = [(data[\"urls\"][i], i) for i in data[\"urls\"].keys()]\n",
        "\n",
        "#On retire les lignes ne contenant pas de lien twitter, on utilise dt pour ça\n",
        "for index in sorted(dt, reverse=True):\n",
        "    del df[index]\n",
        "\n",
        "#append à df des tuples (identifiant, \"NaN\") pour tous les identifiants sans lien Twitter\n",
        "for site in dataset:\n",
        "  if site[0] not in [item for t in df for item in t]:\n",
        "    df.append((site[0], \"NaN\")) #Objectif : Obtenir liste users avec Nom2 et comparaison NLP grâce à Description -> \"twitter.com/\"+username\n",
        "df.sort(reverse=True)\n",
        "\n",
        "#On rajoute la nouvelle colonne ne contenant que les liens au tableau\n",
        "tableauSites['Twitter'] = [item[1] for item in df]\n",
        "\n",
        "tableauSites"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpzkxILxEwCc"
      },
      "source": [
        "#dans dataset remove lignes sans lien Twitter (inutiles) puis chaque ligne prendre nom exact avec lien Twitter et obtenir X tweets (200 par exemple) puis analyse de thème dessus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft3QGW6qJ2dA"
      },
      "source": [
        "import copy\n",
        "save_dataset = copy.deepcopy(dataset)\n",
        "\n",
        "i = 0\n",
        "for i in range(len(save_dataset)):\n",
        "  save_dataset[i] = save_dataset[i] + [df[i][1]]\n",
        "  i += 1\n",
        "\n",
        "save_dataset = [x for x in save_dataset if x[3] != \"NaN\"] #Retire les lignes où pas de lien\n",
        "for line in save_dataset:\n",
        "  if \"twitter.com/\" in line[3]:\n",
        "    if \"?lang=fr\" in line[3]:\n",
        "      size = len(line[3])\n",
        "      line[3] = line[3][12:size - 8].strip('@')\n",
        "    else:\n",
        "      line[3] = line[3][12:].strip('@')\n",
        "# save_dataset = [[x[0],x[1],x[2],x[3][12:len(x[3]) - 8]] for x in save_dataset if \"?lang=fr\" in x[3]:] #Retire les lignes où pas de lien\n",
        "\n",
        "save_dataset\n",
        "\n",
        "tableauSites = pd.DataFrame(save_dataset, columns=['Identifiant','Fiabilité','Description','Twitter'])\n",
        "tableauSites"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m45SAqPiSem3"
      },
      "source": [
        "tableauSites a bien la colonne avec les userID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGMrrdUZjIDd"
      },
      "source": [
        "# tableauSites.to_csv('json1.csv')\n",
        "# !cp json1.csv \"/content/drive/My Drive/PFE\""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh39kNvYlue2"
      },
      "source": [
        "# 1 : site parodique\n",
        "# 2 : non fiable\n",
        "# 3 : site à la fiabilité douteuse\n",
        "# 4 : fiable"
      ],
      "execution_count": 56,
      "outputs": []
    }
  ]
}