{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GetCsvFromJsonOnline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlBVzfD1H4YxdQhLy3c66D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjunjin/PFE-ING3-IA/blob/master/GetCsvFromJsonOnline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsXtMKsf4fvj"
      },
      "source": [
        "Récupération du fichier JSON fourni par lemonde prenant la forme d'un dictionnaire contenant des informations sur différents sites de news.\n",
        "\n",
        "https://www.lemonde.fr/webservice/decodex/updates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4jVrGopjlXl"
      },
      "source": [
        "import urllib.request, json \n",
        "with urllib.request.urlopen(\"https://www.lemonde.fr/webservice/decodex/updates\") as url:\n",
        "    data = json.loads(url.read().decode())"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33BsXDwb5gJX"
      },
      "source": [
        "Création de la liste ***dataset*** contenant pour tous les sites du fichier JSON :\n",
        "*   Leur identifiant\n",
        "*   Leur nom\n",
        "*   Leur niveau de fiabilité\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zbRRRijqmER"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for i in data[\"sites\"].keys():\n",
        "  dataset.append([int(i),data[\"sites\"][i][2],data[\"sites\"][i][0]])"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebDdY14W8yUS"
      },
      "source": [
        "Récupération dans une liste ***df*** de tous les liens Twitter fournis dans le JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDq7gmAG81Xo"
      },
      "source": [
        "import re\n",
        "\n",
        "dt = []\n",
        "compteur = 0\n",
        "for i in data[\"urls\"].keys():\n",
        "  if not(re.search('twitter', i)):\n",
        "    dt.append(compteur)\n",
        "  compteur = compteur+1\n",
        "\n",
        "df = [(data[\"urls\"][i], i) for i in data[\"urls\"].keys()]\n",
        "\n",
        "for index in sorted(dt, reverse=True):\n",
        "    del df[index]\n",
        "\n",
        "for site in dataset:\n",
        "  if site[0] not in [item for t in df for item in t]:\n",
        "    df.append((site[0], \"NaN\"))\n",
        "df.sort(reverse=True)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Jftb_M6q13"
      },
      "source": [
        "Suppression dans ***dataset*** des lignes dont le site ne possède pas de lien twitter conforme.\n",
        "\n",
        "Ajout dans ***dataset*** de la colonne userID contenant le nom d'utilisateur Twitter pour les sites restant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLysik886cwX"
      },
      "source": [
        "import copy\n",
        "save_dataset = copy.deepcopy(dataset)\n",
        "\n",
        "i = 0\n",
        "for i in range(len(save_dataset)):\n",
        "  save_dataset[i] = save_dataset[i] + [df[i][1]]\n",
        "  i += 1\n",
        "\n",
        "save_dataset = [x for x in save_dataset if x[3] != \"NaN\"]\n",
        "\n",
        "for line in save_dataset:\n",
        "  if \"twitter.com/\" in line[3]:\n",
        "    if \"?lang=fr\" in line[3]:\n",
        "      size = len(line[3])\n",
        "      line[3] = line[3][12:size - 8].strip('@')\n",
        "    else:\n",
        "      line[3] = line[3][12:].strip('@')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m45SAqPiSem3"
      },
      "source": [
        "Sauvegarde sur le Drive de ***dataset*** dans un fichier csv\n",
        "\n",
        "Le niveau de fiabilité des sites se lie de cette manière :\n",
        "\n",
        "\n",
        "1.   Site parodique\n",
        "2.   Site non fiable\n",
        "3.   Site à la fiabilité douteuse\n",
        "4.   Site fiable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5aK8pU34xvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a64065b-8ce1-4731-a554-e5f9ce72bc45"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "res = pd.DataFrame(save_dataset, columns=['id','name','trust','userID'])\n",
        "\n",
        "res.to_csv('tableauSites.csv')\n",
        "!cp tableauSites.csv \"/content/drive/My Drive/PFE\""
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}