{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinkWithTwitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjunjin/PFE-ING3-IA/blob/branch1/LinkWithTwitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ndr1ImFEo6w"
      },
      "source": [
        "Installation de la library *python-dotenv*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ1BtietFVX_",
        "outputId": "55642300-58d0-461f-8958-07d3fc410d4d"
      },
      "source": [
        "!pip install python-dotenv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (0.19.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYWW5ymDHlMF"
      },
      "source": [
        "Importation des libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvM0ew0bDEzj"
      },
      "source": [
        "import tweepy\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dotenv import dotenv_values\n",
        "import requests\n",
        "from IPython.display import JSON"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tnZdcHeFysW"
      },
      "source": [
        "Liaison au Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XJZaYlBdx7p",
        "outputId": "da87d5c1-fb90-430d-c8ee-dc6137b973c7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = \"/content/drive/My Drive/PFE/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISVEBKTDF2jm"
      },
      "source": [
        "Récupération dans ***data*** des donnés nécessaires de tableauSites.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV8keOGed_Zd"
      },
      "source": [
        "data = pd.read_csv(DATA_PATH+\"tableauSites.csv\", encoding='utf-8').drop(['Unnamed: 0', 'id'], axis=1).values.tolist()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxAomGqCGFDt"
      },
      "source": [
        "Récupération des clés pour se connecter au compte Twitter Developers et utiliser son API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM4AMrZ1DLZT"
      },
      "source": [
        "API_TOKEN = pd.read_csv(DATA_PATH+\"Clés.csv\", encoding='utf-8', sep=',')['Bearer Token'][0]\n",
        "API_KEY = pd.read_csv(DATA_PATH+\"Clés.csv\", encoding='utf-8', sep=',')['API Key'][0]\n",
        "API_SECRET = pd.read_csv(DATA_PATH+\"Clés.csv\", encoding='utf-8', sep=',')['API Key Secret'][0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEjZKlj2GSzN"
      },
      "source": [
        "Authentifiation au compte Twitter Developers et récupération de l'API tweepy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym4cck9VECa5"
      },
      "source": [
        "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAoDfaEzGvqE"
      },
      "source": [
        "Création d'une classe TwitterUser contenant les fonctions de récupération de données (utilisateurs, tweets, ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1VhX0_ENen"
      },
      "source": [
        "    class TwitterUser():\n",
        "        _screen_name = \"\"\n",
        "        _id = \"\"\n",
        "        _user_data = None\n",
        "        followers = []\n",
        "        following = []\n",
        "        tweet = []\n",
        "        like = []\n",
        "        request_count = 0\n",
        "\n",
        "        def __init__(self, sname = \"\", _json = {}):\n",
        "            if (sname != \"\"):\n",
        "                self._screen_name = sname\n",
        "                self.get_info()\n",
        "            if (_json != {}):\n",
        "                self._json = _json\n",
        "\n",
        "        def get_info(self):\n",
        "            try:\n",
        "                self._user_data = api.get_user(screen_name=self._screen_name)\n",
        "            except Exception as e:\n",
        "                print(\"Exception get_info\")\n",
        "\n",
        "        def get_all_data(self):\n",
        "            self.get_tweets()\n",
        "            # self.get_followers()\n",
        "            # self.get_following()\n",
        "            # self.get_like()\n",
        "\n",
        "        def set_name(self, name):\n",
        "            self._screen_name = name\n",
        "\n",
        "        def set_id(self, _id):\n",
        "            self._id = _id\n",
        "\n",
        "        def get_followers(self):\n",
        "            with tqdm(total=self._user_data.followers_count, desc=\"get_followers for \" + self._user_data.screen_name) as pbar:\n",
        "                for page in tweepy.Cursor(api.followers, screen_name=self._screen_name, count=200).pages():\n",
        "                    self.followers.extend(page)\n",
        "                    pbar.update(len(page))\n",
        "\n",
        "\n",
        "        def get_following(self):\n",
        "            with tqdm(total=self._user_data.friends_count, desc=\"get_following for \" + self._user_data.screen_name) as pbar:\n",
        "                for page in tweepy.Cursor(api.friends, screen_name=self._screen_name, count=200).pages():\n",
        "                    self.following.extend(page)\n",
        "                    pbar.update(len(page))\n",
        "\n",
        "        # on ne peut prendre que les 3000 derniers\n",
        "        def get_tweets(self):\n",
        "            try:\n",
        "                with tqdm(total=1000, desc=\"get_tweets for \" + self._user_data.screen_name) as pbar:\n",
        "                    for page in tweepy.Cursor(api.user_timeline, screen_name=self._screen_name, count=200).pages():\n",
        "                        self.tweet.extend(page)\n",
        "                        pbar.update(len(page))\n",
        "            except Exception as e:\n",
        "                print(\"Exception get_tweets\")\n",
        "\n",
        "        # on ne peut prendre que les 3000 derniers\n",
        "        def get_like(self):\n",
        "            if self._user_data.protected:\n",
        "                print(\"can get like from \" + self._user_data.screen_name + \", the account is protected\")\n",
        "            else:\n",
        "                with tqdm(total=3000, desc=\"get_likes for \" + self._user_data.screen_name) as pbar:\n",
        "                    for page in tweepy.Cursor(api.favorites, screen_name=self._screen_name, count=200).pages():\n",
        "                        self.like.extend(page)\n",
        "                        pbar.update(len(page))\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-U_RpJEp5by"
      },
      "source": [
        "Division de la liste des site en plusieurs sous liste pour ne pas exéder la RAM alloué par Google Colab "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPHt_5sDp32s"
      },
      "source": [
        "def split_list(a_list):\n",
        "    half = len(a_list)//2\n",
        "    return a_list[:half], a_list[half:]\n",
        "\n",
        "dataA, dataB = split_list(data)\n",
        "data1, data2 = split_list(dataA)\n",
        "data3, data4 = split_list(dataB)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfLfTerE4v8w"
      },
      "source": [
        "Ajout de tous les tweets dans 4 tableaux qui vont contenir les données importantes de chaque tweet. Chaque tableau contient un quart des données de ***data***. Les tableaux sont ensuite sauvegardé au format pkl sur le Drive.\n",
        "\n",
        "Pour obtenir chaque morceau il faut relancer la session (ce qui libère la RAM) et exécuter le bloc de commande contenant le morceau voulu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxG51-Y8Qh_e"
      },
      "source": [
        "Création et sauvegarde du tableau 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma3JwJwD4vn6"
      },
      "source": [
        "list_tweet = []\n",
        "\n",
        "for site in data1:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "  for tweet in user.tweet:\n",
        "    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\n",
        "\n",
        "res = pd.DataFrame(list_tweet, columns=['user_id', 'id_tweet', 'tweet', 'retweet_count', 'favorite_count', 'label_liability'])\n",
        "res.to_pickle(DATA_PATH+\"quartier1.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsm2agoEQosG"
      },
      "source": [
        "Création et sauvegarde du tableau 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLHc-ZS9CP0E"
      },
      "source": [
        "list_tweet = []\n",
        "\n",
        "for site in data2:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "  for tweet in user.tweet:\n",
        "    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\n",
        "\n",
        "res = pd.DataFrame(list_tweet, columns=['user_id', 'id_tweet', 'tweet', 'retweet_count', 'favorite_count', 'label_liability'])\n",
        "res.to_pickle(DATA_PATH+\"quartier2.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUMIiSmOQphw"
      },
      "source": [
        "Création et sauvegarde du tableau 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-IGxUhfQsfI"
      },
      "source": [
        "list_tweet = []\n",
        "\n",
        "for site in data2:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "  for tweet in user.tweet:\n",
        "    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\n",
        "\n",
        "res = pd.DataFrame(list_tweet, columns=['user_id', 'id_tweet', 'tweet', 'retweet_count', 'favorite_count', 'label_liability'])\n",
        "res.to_pickle(DATA_PATH+\"quartier3.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmPLcpIKQqdb"
      },
      "source": [
        "Création et sauvegarde du tableau 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV5rrGYlQt1n"
      },
      "source": [
        "list_tweet = []\n",
        "\n",
        "for site in data2:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "  for tweet in user.tweet:\n",
        "    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\n",
        "\n",
        "res = pd.DataFrame(list_tweet, columns=['user_id', 'id_tweet', 'tweet', 'retweet_count', 'favorite_count', 'label_liability'])\n",
        "res.to_pickle(DATA_PATH+\"quartier4.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jV_qISFQvcC"
      },
      "source": [
        "Après avoir obtenu les 4 tableaux en ayant relancé 4 sessions, une pour la création de chaque tableau, on récupère les 4 tableaux."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsAnHYjgQRWe"
      },
      "source": [
        "tableauTweets1 = pd.read_pickle(DATA_PATH+\"quartier1.pkl\")\n",
        "tableauTweets2 = pd.read_pickle(DATA_PATH+\"quartier2.pkl\")\n",
        "tableauTweets3 = pd.read_pickle(DATA_PATH+\"quartier3.pkl\")\n",
        "tableauTweets4 = pd.read_pickle(DATA_PATH+\"quartier4.pkl\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIY_aadkqc58"
      },
      "source": [
        "Concaténation des 4 tableaux en un seul et sauvegarde dans un seul fichier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkkYRw4Urer0"
      },
      "source": [
        "tableauTweets = pd.concat([tableauTweets1,tableauTweets2,tableauTweets3,tableauTweets4])\n",
        "tableauTweets.reset_index(drop=True)\n",
        "tableauTweets.to_pickle(DATA_PATH+\"tableauTweets.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvhGQXIHq8wC"
      },
      "source": [
        "Réinitialisation du fichier PauchBas.pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YGrwhVLp-XM"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(DATA_PATH+\"PaucBas.pickle\", \"wb\") as obj_file:\n",
        "    pickle.dump([], obj_file, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhSJo6vwG8kn"
      },
      "source": [
        "Sauvegarde des données reçu pour chaque utilisateur Twitter recherché"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J7FWlgrrFTx"
      },
      "source": [
        "# with open(DATA_PATH+\"PaucBas.pickle\", \"ab\") as obj_file:\n",
        "#   for site in data1:\n",
        "#     user = TwitterUser(sname= site[2])\n",
        "#     user.get_all_data()\n",
        "#     pickle.dump([user], obj_file, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uqFx3lwrFEU"
      },
      "source": [
        "# with open(DATA_PATH+\"PaucBas.pickle\", \"ab\") as obj_file:\n",
        "#   for site in data2:\n",
        "#     user = TwitterUser(sname= site[2])\n",
        "#     user.get_all_data()\n",
        "#     pickle.dump([user], obj_file, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}