{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinkWithTwitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjunjin/PFE-ING3-IA/blob/master/LinkWithTwitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ndr1ImFEo6w"
      },
      "source": [
        "Installation de la library *python-dotenv*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ1BtietFVX_",
        "outputId": "ec95fee7-4288-45ea-9129-f8b8dbaa79d2"
      },
      "source": [
        "!pip install python-dotenv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.19.2-py2.py3-none-any.whl (17 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYWW5ymDHlMF"
      },
      "source": [
        "Importation des libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvM0ew0bDEzj"
      },
      "source": [
        "import tweepy\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dotenv import dotenv_values\n",
        "import requests\n",
        "from IPython.display import JSON"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tnZdcHeFysW"
      },
      "source": [
        "Liaison au Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XJZaYlBdx7p",
        "outputId": "e8601fec-e719-4a02-d44d-8ef072ab2cc7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = \"/content/drive/My Drive/PFE/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISVEBKTDF2jm"
      },
      "source": [
        "Récupération dans ***data*** des donnés nécessaires de tableauSites.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV8keOGed_Zd"
      },
      "source": [
        "data = pd.read_csv(DATA_PATH+\"tableauSites.csv\", encoding='utf-8').drop(['Unnamed: 0', 'id'], axis=1).values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vérification"
      ],
      "metadata": {
        "id": "HMnURYGpGF8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqt2dolttWrW",
        "outputId": "d9327519-84e7-421d-e9ed-408cef65573a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['France Soir', 2, 'france_soir'],\n",
              " ['Régénère (Thierry Casasnovas)', 2, 'ThierryRegenere'],\n",
              " ['Silvano Trotta', 2, 'silvano_trotta'],\n",
              " ['Ema Krusi', 2, 'emakrusi'],\n",
              " ['Salim Laïbi (Le Libre Penseur)', 2, 'llp_le_vrai'],\n",
              " ['https://covidinfos.net/', 2, 'covid_infos'],\n",
              " ['Carlo Alberto Brusa', 3, 'cab2626'],\n",
              " ['La Mentable', 1, 'lamentableFR'],\n",
              " ['Elishean-Aufeminin.com', 2, 'Elishean'],\n",
              " ['NoSignalFound', 2, 'DrMo7oG'],\n",
              " ['Les chroniques du Yéti', 2, 'yetiblog'],\n",
              " ['Le Politique', 3, 'lepolitique_fr'],\n",
              " ['Les Répliques', 3, 'les_repliques'],\n",
              " ['Damocles', 3, 'damocles_fr'],\n",
              " ['Jnewsfoot', 1, 'jnewsfoot'],\n",
              " [\"Le Journal de L'Elysée\", 1, 'JournalElysee'],\n",
              " ['20 secondes', 1, '20secondesfr'],\n",
              " ['Golf Investigation', 1, 'Golfinvestigat1'],\n",
              " ['Institut International Indépendant pour la Promotion de la Recherche Scientifique',\n",
              "  1,\n",
              "  'iiiprs'],\n",
              " ['Vox', 4, 'voxdotcom'],\n",
              " [\"L'Usine Nouvelle\", 4, 'usinenouvelle'],\n",
              " ['Les Observateurs de France 24', 4, 'Observateurs'],\n",
              " ['Radio Télévision Suisse', 4, 'radiotelesuisse'],\n",
              " ['RTBF', 4, 'rtbf'],\n",
              " ['La Libre Belgique', 4, 'lalibrebe'],\n",
              " ['Brut', 4, 'brutofficiel'],\n",
              " ['Epoch Times', 4, 'EpochTimesFR'],\n",
              " ['The Conversation', 4, 'FR_Conversation'],\n",
              " ['Actu17', 4, 'actu17'],\n",
              " ['Science Post', 4, 'sciencepost_fr'],\n",
              " ['Révolution permanente', 3, 'RevPermanente'],\n",
              " ['Apr-News', 3, 'APRNEWS1'],\n",
              " ['Afripulse', 2, 'AfriPulse_FR'],\n",
              " ['Whatsupic', 2, 'WhatsupicFr'],\n",
              " ['Suavelos.eu', 2, 'suavelos_fr'],\n",
              " ['Made in Belgium', 2, 'LBDTSS'],\n",
              " ['Le journal news', 1, 'le_journalnews'],\n",
              " ['La Fraîche Gazette', 1, 'FraicheGazette'],\n",
              " ['LaFrance.co', 2, 'onaimelafrance'],\n",
              " ['The Daily Express', 3, 'daily_express'],\n",
              " ['Le Désavantage', 1, 'LeDesavantage'],\n",
              " ['Liberaphion', 1, 'Liberaphion'],\n",
              " ['Le blog de Thomas Joly', 2, 'thomasjoly60'],\n",
              " ['Euroscoop.fr', 2, 'EuroScoop_FR'],\n",
              " [\"Dr Wakefield's Work Must Continue\", 2, 'drwakefield'],\n",
              " ['Allo santé', 2, 'allosante2016'],\n",
              " [\"L'important\", 4, 'limportant_fr'],\n",
              " ['http://www.worldtvdesinfo.com', 1, 'worldtvdesinfo'],\n",
              " ['AkhnaPress.com', 1, 'akhnapress'],\n",
              " ['Le Ouest-Franc', 1, 'leouestfranc'],\n",
              " ['Alertes Infos', 4, 'alertesinfos'],\n",
              " ['http://www.thegatewaypundit.com/', 2, 'gatewaypundit'],\n",
              " ['Alain Soral', 2, 'alainsoraloffic'],\n",
              " ['David Van Hemelryck', 2, 'david_vanh'],\n",
              " ['Paul Joseph Watson', 2, 'prisonplanet'],\n",
              " ['Nos Médias', 2, 'nosmedias_fr'],\n",
              " ['Natural News', 2, 'healthranger'],\n",
              " [\"Délit d'images\", 2, 'delitdimages'],\n",
              " ['La sirène du Pays Basque', 1, 'LaSireneBasque'],\n",
              " [\"L'oignon du cantal\", 1, 'OignonCantal'],\n",
              " ['Wake up call', 2, 'wucnews'],\n",
              " ['Le Week', 3, 'leweekofficiel'],\n",
              " ['Tomimag', 1, 'tomimagfr'],\n",
              " [\"On sait ce qu'on veut qu'on sache\", 2, 'onsaitceque'],\n",
              " ['Buzzarena', 3, 'BuzzArenaFr'],\n",
              " ['Actumag', 3, 'actumaginfo'],\n",
              " ['francetvdesinfo.fr', 1, 'francetvdesinfo'],\n",
              " ['Astuce du jour', 2, 'AstuceDuJourMag'],\n",
              " ['Actualidad panamericana', 1, 'actualidadpanam'],\n",
              " ['Daily Snark', 1, 'DailySnark'],\n",
              " ['Free Wood Post', 1, 'freewoodpost'],\n",
              " ['The Postillon', 1, 'The_Postillon'],\n",
              " ['The Daily Mash', 1, 'thedailymash'],\n",
              " ['The Akami Shimbun', 1, 'akami_shimbun'],\n",
              " ['Alex Jones', 2, 'RealAlexJones'],\n",
              " ['Ravelations', 1, 'ravelationsmag'],\n",
              " ['Investigaction', 2, 'InvestigAction'],\n",
              " ['Alertes News', 3, 'AlertesNews'],\n",
              " ['Cyceon', 3, 'cyceon_fr'],\n",
              " ['Gamekult', 4, 'gamekult'],\n",
              " ['Causeur', 4, 'causeur'],\n",
              " ['Secret News', 1, 'SecretNewsInfo'],\n",
              " ['Hors de vos pensés', 2, 'envie_devivre'],\n",
              " ['Ridicule TV', 2, 'RidiculeTV'],\n",
              " ['Filteris', 3, 'Filteris'],\n",
              " ['Europe Israël', 2, 'Europe_Israel'],\n",
              " ['Civil War In Europe', 2, 'Civilwarineurop'],\n",
              " ['The spirit science', 2, 'spiritsciences'],\n",
              " ['Radio Cockpit', 1, 'radiocockpit'],\n",
              " ['Corse Machin', 1, 'corsemachin'],\n",
              " ['BuzzBeed', 1, 'BuzzBeed'],\n",
              " ['Fox News', 3, 'FoxNews'],\n",
              " ['i24 News', 4, 'i24NEWS_FR'],\n",
              " ['Les Jours', 4, 'Lesjoursfr'],\n",
              " ['Nextinpact', 4, 'nextinpact'],\n",
              " ['Rue89 Bordeaux', 4, 'Rue89Bordeaux'],\n",
              " ['Le Soir', 4, 'lesoir'],\n",
              " ['Conspiracy Watch', 4, 'conspiration'],\n",
              " ['Korben', 4, 'Korben'],\n",
              " [\"Le Lab d'Europe 1\", 4, 'leLab_E1'],\n",
              " ['Quotidien', 4, 'Qofficiel'],\n",
              " ['Basta !', 4, 'Bastamag'],\n",
              " ['Lundi Matin', 4, 'lundimat1'],\n",
              " ['Fakir', 4, 'Fakir_'],\n",
              " ['Le blog de Paul Jorion', 4, 'PaulJorion'],\n",
              " ['Les Cahiers du Football', 4, 'cahiersdufoot'],\n",
              " ['Acrimed', 4, 'acrimed_info'],\n",
              " ['Charlie Hebdo', 4, 'Charlie_Hebdo_'],\n",
              " ['E-sante', 4, 'esantefr'],\n",
              " ['Passeport Santé', 4, 'PasseportSante'],\n",
              " ['Blog Morandini Santé', 3, 'morandinisante'],\n",
              " ['Centre d’études et de documentation du sucre (CEDUS)', 3, 'lesucretvous'],\n",
              " ['Electrosensible.info', 4, 'electroallergik'],\n",
              " ['Allo Docteurs', 4, 'allodocteurs'],\n",
              " ['Alternatives Economiques', 4, 'AlterEco_'],\n",
              " ['Mes Opinions', 4, 'mesopinions_com'],\n",
              " ['Change.org', 4, 'Change'],\n",
              " ['Buzzly', 3, 'buzzlyweb'],\n",
              " ['Canard PC', 4, 'Canardpcredac'],\n",
              " ['Confidentielles', 4, 'Confidentielles'],\n",
              " ['Dirty Biology', 4, 'dirtybiology'],\n",
              " ['e-penser (Bruce Benamran)', 4, 'epenser'],\n",
              " ['Clubic', 4, 'Clubic'],\n",
              " ['Le journal des femmes', 4, 'journalDfemmes'],\n",
              " ['Comment ça marche', 4, 'commentcamarche'],\n",
              " ['Jeuxvideo.com', 4, 'JVCom'],\n",
              " ['Astuces naturelles', 2, 'astucenaturelle'],\n",
              " ['Santé + magazine', 2, 'Santeplusmag'],\n",
              " ['Jean-Paul Ney', 2, 'jpney'],\n",
              " [\"L'humour de droite\", 1, 'humourdedroite'],\n",
              " ['Maître Eolas', 4, 'maitre_eolas'],\n",
              " ['Yagg', 4, 'Yagg'],\n",
              " ['Vosges Matin', 4, 'VosgesMatin'],\n",
              " ['Vice France', 4, 'VICEfr'],\n",
              " ['Vice', 4, 'VICE'],\n",
              " ['USA Today', 4, 'USATODAY'],\n",
              " ['TV Magazine', 4, 'TVMAG'],\n",
              " ['The Washington Post', 4, 'washingtonpost'],\n",
              " ['The Wall Street Journal', 4, 'wsj'],\n",
              " ['The New York Times', 4, 'nytimes'],\n",
              " ['The Intercept', 4, 'theintercept'],\n",
              " ['The Independent', 4, 'Independent'],\n",
              " ['The Herald', 4, 'heraldscotland'],\n",
              " ['The Guardian', 4, 'guardian'],\n",
              " ['The Economist', 4, 'TheEconomist'],\n",
              " ['The Daily Telegraph', 4, 'dailytelegraph'],\n",
              " ['TF1', 4, 'TF1'],\n",
              " ['Têtu', 4, 'TETUmag'],\n",
              " ['Télérama', 4, 'Telerama'],\n",
              " ['Télé Loisirs', 4, 'TeleLoisirs'],\n",
              " ['Télé 7 jours', 4, 'Tele7'],\n",
              " ['Sud Ouest', 4, 'sudouest'],\n",
              " ['Streetpress', 4, 'streetpress'],\n",
              " ['Snopes', 4, 'snopes'],\n",
              " ['Slate', 4, 'Slatefr'],\n",
              " ['Sciences et Avenir', 4, 'Sciences_Avenir'],\n",
              " ['Science et Vie', 4, 'science_et_vie'],\n",
              " ['Rue89', 4, 'Rue89'],\n",
              " ['RTL', 4, 'RTLFrance'],\n",
              " ['RFI', 4, 'RFI'],\n",
              " ['Reuters', 4, 'Reuters'],\n",
              " ['Reporterre', 4, 'Reporterre'],\n",
              " ['Psychologies magazine', 4, 'Psychologies_'],\n",
              " ['Presse-Océan', 4, 'presseocean'],\n",
              " ['Première', 4, 'PremiereFR'],\n",
              " ['Politis', 4, 'Politis_fr'],\n",
              " ['Politico Europe', 4, 'POLITICOEurope'],\n",
              " ['Politico', 4, 'politico'],\n",
              " ['Pèlerin', 4, 'pelerincom'],\n",
              " ['Paris-Normandie', 4, 'paris_normandie'],\n",
              " ['Paris Match', 4, 'ParisMatch'],\n",
              " ['Ouest-France', 4, 'OuestFrance'],\n",
              " ['Onze mondial', 4, 'OnzeMondial'],\n",
              " ['Numerama', 4, 'Numerama'],\n",
              " ['Notre temps', 4, 'NotreTemps'],\n",
              " ['Nord éclair', 4, 'NordEclairWeb'],\n",
              " ['Non-stop-politique', 4, 'nonspolitique'],\n",
              " ['Nice-Matin', 4, 'Nice_Matin'],\n",
              " ['New York Daily News', 4, 'NYDailyNews'],\n",
              " [\"Mouv'\", 4, 'mouv'],\n",
              " ['Mieux vivre votre argent', 4, 'VotreArgent'],\n",
              " ['Midi Libre', 4, 'Midilibre'],\n",
              " ['Mediapart', 4, 'mediapart'],\n",
              " ['Mashable avec France 24', 4, 'MashableFR'],\n",
              " ['Mashable', 4, 'mashable'],\n",
              " ['Marie France', 4, 'mariefrancemag'],\n",
              " ['Marie Claire', 4, 'marieclaire'],\n",
              " ['Marianne', 4, 'MarianneleMag'],\n",
              " ['Madame Figaro', 4, 'Madamefigaro'],\n",
              " ['M6', 4, 'M6'],\n",
              " ['Los Angeles Times', 4, 'latimes'],\n",
              " ['Libération', 4, 'libe'],\n",
              " ['Les Inrockuptibles', 4, 'lesinrocks'],\n",
              " ['Les Echos', 4, 'LesEchos'],\n",
              " ['Le Temps', 4, 'letemps'],\n",
              " ['Le Télégramme', 4, 'LeTelegramme'],\n",
              " ['Le Progrès', 4, 'Le_Progres'],\n",
              " ['Le Populaire du Centre', 4, 'lepopulaire_fr'],\n",
              " ['Le Point', 4, 'LePoint'],\n",
              " ['Le Parisien-Aujourd’hui en France', 4, 'le_Parisien'],\n",
              " ['Le Nouvel Economiste', 4, 'LeNouvelEco'],\n",
              " ['Le Monde diplomatique', 4, 'mdiplo'],\n",
              " ['Le Monde', 4, 'lemondefr'],\n",
              " ['Le Maine Libre', 4, 'lemainelibre'],\n",
              " ['Le Journal du dimanche', 4, 'leJDD'],\n",
              " ['Le Journal du Centre', 4, 'lejdc_fr'],\n",
              " ['Le Journal de Saône-et-Loire', 4, 'lejsl'],\n",
              " ['Le Figaro', 4, 'Le_Figaro'],\n",
              " ['Le Dauphiné libéré', 4, 'ledauphine'],\n",
              " ['Le Courrier picard', 4, 'CourrierPicard'],\n",
              " ['Le Bondy Blog', 4, 'LeBondyBlog'],\n",
              " ['Le Bien public', 4, 'Lebienpublic'],\n",
              " ['Le Berry républicain', 4, 'leberry_fr'],\n",
              " ['LCP', 4, 'LCP'],\n",
              " ['LCI', 4, 'LCI'],\n",
              " ['La Voix du Nord', 4, 'lavoixdunord'],\n",
              " ['La Tribune', 4, 'LT_LaTribune'],\n",
              " ['La République du Centre', 4, 'larep_fr'],\n",
              " ['La Repubblica', 4, 'repubblicait'],\n",
              " ['La Provence', 4, 'laprovence'],\n",
              " ['La Montagne', 4, 'lamontagne_fr'],\n",
              " ['La Marseillaise', 4, 'lamarsweb'],\n",
              " ['La Dépêche du Midi', 4, 'ladepechedumidi'],\n",
              " ['La Croix', 4, 'LaCroix'],\n",
              " [\"L'Union\", 4, 'UnionArdennais'],\n",
              " [\"L'Opinion\", 4, 'Lopinion_fr'],\n",
              " [\"L'Obs\", 4, 'lobs'],\n",
              " [\"L'Internaute\", 4, 'linternautemag'],\n",
              " [\"L'Indépendant\", 4, 'lindependant'],\n",
              " [\"L'Humanité\", 4, 'humanite_fr'],\n",
              " [\"L'Express\", 4, 'LEXPRESS'],\n",
              " [\"L'Est-Eclair\", 4, 'lesteclair'],\n",
              " [\"L'Est républicain\", 4, 'lestrepublicain'],\n",
              " [\"L'Equipe\", 4, 'lequipe'],\n",
              " [\"L'Echo républicain\", 4, 'lecho_fr'],\n",
              " [\"L'Echo\", 4, 'lecho'],\n",
              " [\"L'Alsace-Le Pays\", 4, 'lalsace'],\n",
              " ['CNews', 4, 'cnews'],\n",
              " ['Irish Daily Star', 4, 'IsFearrAnStar'],\n",
              " ['Huffington Post', 4, 'LeHuffPost'],\n",
              " ['Houston Chronicle', 4, 'HoustonChron'],\n",
              " ['Hoaxbuster', 4, 'hoaxbuster'],\n",
              " ['Hoax Slayer', 4, 'hoaxslayer'],\n",
              " ['Historia', 4, 'historiamag'],\n",
              " ['Grazia', 4, 'Grazia'],\n",
              " ['Glamour', 4, 'glamourmag'],\n",
              " ['Franceinfo', 4, 'franceinfo'],\n",
              " ['France Inter', 4, 'franceinter'],\n",
              " ['France Culture', 4, 'franceculture'],\n",
              " ['France Bleu', 4, 'francebleu'],\n",
              " ['France 5', 4, 'France5tv'],\n",
              " ['France 3', 4, 'France3tv'],\n",
              " ['France 24', 4, 'FRANCE24'],\n",
              " ['France 2', 4, 'France2tv'],\n",
              " ['Financial Times', 4, 'FinancialTimes'],\n",
              " ['Femme actuelle', 4, 'femmeactuelle'],\n",
              " ['Europe 1', 4, 'Europe1'],\n",
              " ['Euronews', 4, 'euronews'],\n",
              " ['Cnews Matin', 4, 'cnewsmatin'],\n",
              " ['Der Standard', 4, 'derStandardat'],\n",
              " ['Der Spiegel', 4, 'DerSPIEGEL'],\n",
              " ['Denver Post', 4, 'denverpost'],\n",
              " ['Debunkers de hoax', 4, 'DebunkerHED'],\n",
              " ['Courrier international', 4, 'courrierinter'],\n",
              " ['Cosmopolitan', 4, 'Cosmopolitan'],\n",
              " ['Corse-Matin', 4, 'Corse_Matin'],\n",
              " ['Corriere della Sera', 4, 'Corriere'],\n",
              " ['CNN', 4, 'CNN'],\n",
              " ['Chicago Tribune', 4, 'chicagotribune'],\n",
              " ['Charente libre', 4, 'charentelibre'],\n",
              " ['Challenges', 4, 'Challenges'],\n",
              " ['Capital', 4, 'MagazineCapital'],\n",
              " ['Canal+', 4, 'canalplus'],\n",
              " ['BuzzFeed', 4, 'BuzzFeed'],\n",
              " ['Business Insider', 4, 'businessinsider'],\n",
              " ['Boursorama', 4, 'Boursorama'],\n",
              " ['Bloomberg', 4, 'Bloomberg'],\n",
              " ['Biba', 4, 'bibamagazine'],\n",
              " ['BFMTV', 4, 'BFMTV'],\n",
              " ['BBC', 4, 'BBC'],\n",
              " ['Atlantico', 4, 'atlantico_fr'],\n",
              " ['Associated Press', 4, 'AP'],\n",
              " ['Arte', 4, 'ARTEfr'],\n",
              " ['Arrêt sur images', 4, 'arretsurimages'],\n",
              " ['Ameli', 4, 'ameli_actu'],\n",
              " ['Alternet', 4, 'AlterNet'],\n",
              " ['AJ+', 4, 'ajplus'],\n",
              " ['Al Jazeera', 4, 'AJENews'],\n",
              " ['AFP (Agence France-Presse)', 4, 'afpfr'],\n",
              " ['ABC', 4, 'ABC'],\n",
              " ['20 Minutes', 4, '20Minutes'],\n",
              " ['Valeurs actuelles', 3, 'Valeurs'],\n",
              " ['The Daily Beast', 4, 'thedailybeast'],\n",
              " ['Jean-Marc Morandini', 3, 'morandiniblog'],\n",
              " ['Gentside', 3, 'Gentside'],\n",
              " ['Ohmymag', 3, 'ohmymagfr'],\n",
              " ['Les éconoclastes', 3, 'leseconoclastes'],\n",
              " ['TV Libertés', 2, 'tvlofficiel'],\n",
              " ['Topito', 4, 'topito_com'],\n",
              " ['The Other 98%', 3, 'other98'],\n",
              " ['The Mind Unleashed', 2, 'UnleashMind'],\n",
              " ['Tariq Ramadan', 4, 'TariqRamadan'],\n",
              " ['Sputniknews', 3, 'sputnik_fr'],\n",
              " ['Spi0n', 4, 'spi0n'],\n",
              " ['Sante Ici', 2, 'santeici'],\n",
              " ['Russia Today (version française)', 3, 'RTenfrancais'],\n",
              " ['Russia Today', 3, 'RT_com'],\n",
              " ['Ruptly', 3, 'ruptly'],\n",
              " ['Public', 4, 'PublicFR'],\n",
              " ['Positivr', 4, 'ThePOSITIVR'],\n",
              " ['Pause Cafein', 3, 'PauseCafein'],\n",
              " ['Oumma', 3, 'oumma'],\n",
              " ['OnlineMagazin', 3, 'OnlineMagazin'],\n",
              " ['OJIM', 3, 'ojim_france'],\n",
              " ['Occupy Democrats', 3, 'occupydemocrats'],\n",
              " ['Novopress', 3, 'novopress'],\n",
              " ['Nouvelles de France', 3, 'ndffr'],\n",
              " ['New York Post', 3, 'nypost'],\n",
              " ['Mr Mondialisation', 4, 'Mondi_Alisation'],\n",
              " ['MinuteBuzz', 4, 'minutebuzz'],\n",
              " ['MetaTV', 3, 'MetaTVFrance'],\n",
              " ['Melty', 4, 'melty_fr'],\n",
              " ['Les News', 3, 'LesNews'],\n",
              " ['Le Tribunal du Net', 2, 'Letribunaldunet'],\n",
              " ['Le Journal du siècle', 3, 'JournalduSiecle'],\n",
              " ['La Zappeuse Folle', 3, 'zapeuzefolle'],\n",
              " ['Konbini', 4, 'KonbiniFr'],\n",
              " ['JSS News', 3, 'JSSNews'],\n",
              " ['Infos 140', 3, 'actu140'],\n",
              " ['Humour de mecs', 3, 'HumourDeMec'],\n",
              " ['Hitek', 3, 'Hitekfr'],\n",
              " ['Herault Tribune', 4, 'HeraultTribune'],\n",
              " ['Fdesouche', 3, 'F_Desouche'],\n",
              " ['Etienne Chouard', 3, 'Etienne_Chouard'],\n",
              " ['Enquête & Débat', 3, 'Enquete_etDebat'],\n",
              " ['Drudge Report', 3, 'DRUDGE_REPORT'],\n",
              " ['Doctissimo', 4, 'doctissimo'],\n",
              " ['Demotivateur', 3, 'Demotivateur'],\n",
              " ['Daily Mail', 3, 'MailOnline'],\n",
              " ['Counterpunch', 3, 'NatCounterPunch'],\n",
              " ['Contribuables', 3, 'contribuables'],\n",
              " ['Contrepoints', 3, 'Contrepoints'],\n",
              " ['Buzzfil', 3, 'BuzzFilcom'],\n",
              " ['Breitbart', 3, 'BreitbartNews'],\n",
              " ['Boulevard Voltaire', 3, 'BVoltaire'],\n",
              " ['Bipartisan Report', 3, 'bipartisanism'],\n",
              " [\"Before it's news\", 2, 'beforeitsnews'],\n",
              " ['American News X', 3, 'americannewsx'],\n",
              " ['Al Kanz', 4, 'Alkanz'],\n",
              " ['Agence Info Libre', 3, 'AgenceInfoLibre'],\n",
              " ['Médias Presse Info', 2, 'InfoMdia'],\n",
              " ['Dreuz Info', 2, 'Dreuz_1fo'],\n",
              " ['Information Liberation', 2, 'infolibnews'],\n",
              " ['Yann Merkado', 2, 'YannMerkado'],\n",
              " ['Wikistrike', 2, 'WikistrikeW'],\n",
              " ['Wanted Pedo', 2, 'WantedPedo'],\n",
              " ['Tprincedelamour', 2, 'tprincedelamour'],\n",
              " ['SwagActu', 2, 'SwagActu'],\n",
              " ['Russia Insider', 2, 'RussiaInsider'],\n",
              " ['Riposte laïque', 2, '1RiposteLaique'],\n",
              " ['Réseau International', 2, 'reseau_internat'],\n",
              " ['Réinformation RC', 2, 'ReinformationRC'],\n",
              " ['Red State Watcher', 2, 'RedState'],\n",
              " ['Paris Vox', 2, 'paris_vox'],\n",
              " ['Panamza', 2, 'Panamza'],\n",
              " ['Nord Actu', 2, 'nord_actu'],\n",
              " ['Nice Provence Info', 2, 'NiceProvenceInf'],\n",
              " ['LesObservateurs.ch', 2, 'ObservateursCH'],\n",
              " ['Les Crises', 3, 'oberruyer'],\n",
              " ['Le site du Professeur Henri Joyeux', 2, 'PrJoyeux'],\n",
              " [\"La gauche m'a tuer\", 2, 'LaGaucheMaTuer'],\n",
              " [\"L'insoumis\", 2, 'linsoumis_fr'],\n",
              " [\"L'Informatrice\", 2, 'linformatrice'],\n",
              " ['Jovanovic', 2, 'pierrejovanovic'],\n",
              " ['Jeune Nation', 2, 'JeuneNation'],\n",
              " ['Infos Bordeaux', 2, 'infosbordeaux'],\n",
              " ['Infowars', 2, 'infowars'],\n",
              " ['Info Contre Info', 2, 'infovsinfo'],\n",
              " ['Hollande Démission', 2, 'David_vanH'],\n",
              " ['Hollande dégage', 2, 'degage_hollande'],\n",
              " ['Hérisson Dissident', 2, 'Herissident'],\n",
              " ['Henry Makow', 2, 'HenryMakow'],\n",
              " ['Fawkes News', 2, 'guy_fawkes_news'],\n",
              " ['Egalité et Réconciliation', 2, 'EetR_National'],\n",
              " ['Diktacratie', 2, 'diktacratie'],\n",
              " ['Dieudosphere (Dieudonné)', 2, 'MbalaDieudo'],\n",
              " ['David Icke', 2, 'davidicke'],\n",
              " ['DailyNewsBin', 2, 'DailyNewsBin'],\n",
              " ['Corbett Report', 2, 'corbettreport'],\n",
              " ['Breizh Info', 2, 'breizh_info'],\n",
              " ['Boris Le Lay', 2, 'boris_lay'],\n",
              " ['Bare Naked Islam', 2, 'barenakedislam'],\n",
              " ['Antipresse', 2, 'antipresse_net'],\n",
              " ['AE911truth', 2, 'AE911Truth'],\n",
              " ['21st Century Wire', 2, '21WIRE'],\n",
              " ['2012 un nouveau paradigme', 2, 'LNParadigme'],\n",
              " ['The Onion', 1, 'theonion'],\n",
              " ['Nordpresse', 1, 'nordpresse'],\n",
              " ['News Thump', 1, 'newsthump'],\n",
              " ['Le Navet', 1, 'Le_Navet'],\n",
              " ['Le Jurafi', 1, 'LeJurafi'],\n",
              " ['Le Gorafi', 1, 'le_gorafi'],\n",
              " ['La Sardine du Port', 1, 'sardinedp'],\n",
              " [\"L'Epique\", 1, 'iepique'],\n",
              " [\"L'Echo de la Boucle\", 1, 'echodelaboucle'],\n",
              " ['Huzlers', 1, 'thehuzlers'],\n",
              " ['Football France', 1, 'FootballFrance_'],\n",
              " ['El Manchar', 1, 'el_manchar'],\n",
              " ['Edukactus', 1, 'edukactus'],\n",
              " ['Clickhole', 1, 'clickhole'],\n",
              " ['Brave Patrie', 1, 'bravepatrie'],\n",
              " ['Branched', 1, 'Branchednews'],\n",
              " ['Boulevard69', 1, 'Boulevard69'],\n",
              " ['Boucherie Ovalie', 1, 'boucherieovalie'],\n",
              " ['Bilboquet Magazine', 1, 'BilboquetMag'],\n",
              " ['Amplifying Glass', 1, 'amplifyingG'],\n",
              " ['24matin.ch', 1, '24matin'],\n",
              " ['Yahoo!', 4, 'Yahoo'],\n",
              " ['Twitter', 4, 'twitter.com'],\n",
              " [\"Le Plus de l'Obs\", 4, 'LePlus'],\n",
              " ['Le FigaroVox', 4, 'FigaroVox'],\n",
              " ['Agoravox', 4, 'agoravox'],\n",
              " ['9gag', 4, '9GAG'],\n",
              " ['Wikipédia', 4, 'Wikipedia'],\n",
              " ['Wikihow', 4, 'wikiHow']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxAomGqCGFDt"
      },
      "source": [
        "Récupération des clés pour se connecter au compte Twitter Developers et utiliser son API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM4AMrZ1DLZT"
      },
      "source": [
        "API_TOKEN = pd.read_csv(DATA_PATH+\"Clés.csv\", encoding='utf-8', sep=',')['Bearer Token'][0]\n",
        "API_KEY = pd.read_csv(DATA_PATH+\"Clés.csv\", encoding='utf-8', sep=',')['API Key'][0]\n",
        "API_SECRET = pd.read_csv(DATA_PATH+\"Clés.csv\", encoding='utf-8', sep=',')['API Key Secret'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEjZKlj2GSzN"
      },
      "source": [
        "Authentifiation au compte Twitter Developers et récupération de l'API tweepy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym4cck9VECa5"
      },
      "source": [
        "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction pour récupérer les tweets d'un utilisateurs, basé sur une solution disponible sur :\n",
        "\n",
        "https://towardsdatascience.com/collect-data-from-twitter-a-step-by-step-implementation-using-tweepy-7526fff2cb31"
      ],
      "metadata": {
        "id": "atQ2cj2SQIx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tweepy import Cursor # Used to perform pagination\n",
        "\n",
        "# Function creating final list\n",
        "def get_tweets_from_user(twitter_user_name, liability, api, page_limit=15, count_tweet=200):\n",
        "    \"\"\"\n",
        "    @params:\n",
        "        - twitter_user_name: the twitter username of a user (company, etc.)\n",
        "        - page_limit: the total number of pages (max=16)\n",
        "        - count_tweet: maximum number to be retrieved from a page\n",
        "        \n",
        "    @return\n",
        "        - all the tweets from the user twitter_user_name\n",
        "    \"\"\"\n",
        "    liability_label = ['Site parodique', 'Site non fiable', 'Site à la fiabilité douteuse', 'Site réputé fiable']\n",
        "    all_tweets = []\n",
        "    try:\n",
        "      print(\"Récupération des tweets de \"+twitter_user_name)\n",
        "    \n",
        "      for page in Cursor(api.user_timeline,\n",
        "                         screen_name=twitter_user_name, \n",
        "                         count=count_tweet).pages(page_limit):\n",
        "        for tweet in page:\n",
        "          parsed_tweet = {}\n",
        "          parsed_tweet['user_id'] = tweet.user.id\n",
        "          parsed_tweet['user_screen_name'] = twitter_user_name\n",
        "          parsed_tweet['id_tweet'] = tweet.id\n",
        "          parsed_tweet['created_at'] = tweet.created_at\n",
        "          parsed_tweet['tweet'] = tweet.text\n",
        "          parsed_tweet['retweet_count'] = tweet.retweet_count\n",
        "          parsed_tweet['favorite_count'] = tweet.favorite_count\n",
        "          parsed_tweet['liability'] = liability\n",
        "          parsed_tweet['liability_label'] = liability_label[liability-1]\n",
        "          parsed_tweet['count_followers'] = tweet.user.followers_count\n",
        "\n",
        "          all_tweets.append(parsed_tweet)\n",
        "\n",
        "      return all_tweets\n",
        "    except Exception as e:\n",
        "      print(\"Exception get_tweets pour \"+twitter_user_name)\n",
        "      return all_tweets"
      ],
      "metadata": {
        "id": "R9r_uSsEP_ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Récupération des tweets"
      ],
      "metadata": {
        "id": "RhHU64ZnFvsR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma3JwJwD4vn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02de8eb2-52cd-4e35-df4c-223e2da5098f"
      },
      "source": [
        "nb = 0\n",
        "list_tweet = []\n",
        "\n",
        "for site in data:\n",
        "  list_tweet = list_tweet + get_tweets_from_user(site[2], site[1], api)\n",
        "  nb += 1\n",
        "  if (nb%10 == 0):\n",
        "    print(\"Avancement : \"+str(nb)+\"/\"+str(len(data)))\n",
        "\n",
        "res = pd.DataFrame(list_tweet, columns=['user_id', 'user_screen_name', 'id_tweet', 'created_at', 'tweet', 'retweet_count', 'favorite_count', 'liability', 'liability_label', 'count_followers'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Récupération des tweets de france_soir\n",
            "Récupération des tweets de ThierryRegenere\n",
            "Récupération des tweets de silvano_trotta\n",
            "Exception get_tweets pour silvano_trotta\n",
            "Récupération des tweets de emakrusi\n",
            "Récupération des tweets de llp_le_vrai\n",
            "Récupération des tweets de covid_infos\n",
            "Récupération des tweets de cab2626\n",
            "Récupération des tweets de lamentableFR\n",
            "Récupération des tweets de Elishean\n",
            "Récupération des tweets de DrMo7oG\n",
            "Avancement : 10/425\n",
            "Récupération des tweets de yetiblog\n",
            "Récupération des tweets de lepolitique_fr\n",
            "Récupération des tweets de les_repliques\n",
            "Récupération des tweets de damocles_fr\n",
            "Exception get_tweets pour damocles_fr\n",
            "Récupération des tweets de jnewsfoot\n",
            "Récupération des tweets de JournalElysee\n",
            "Récupération des tweets de 20secondesfr\n",
            "Exception get_tweets pour 20secondesfr\n",
            "Récupération des tweets de Golfinvestigat1\n",
            "Récupération des tweets de iiiprs\n",
            "Récupération des tweets de voxdotcom\n",
            "Avancement : 20/425\n",
            "Récupération des tweets de usinenouvelle\n",
            "Récupération des tweets de Observateurs\n",
            "Récupération des tweets de radiotelesuisse\n",
            "Récupération des tweets de rtbf\n",
            "Récupération des tweets de lalibrebe\n",
            "Récupération des tweets de brutofficiel\n",
            "Récupération des tweets de EpochTimesFR\n",
            "Récupération des tweets de FR_Conversation\n",
            "Récupération des tweets de actu17\n",
            "Récupération des tweets de sciencepost_fr\n",
            "Avancement : 30/425\n",
            "Récupération des tweets de RevPermanente\n",
            "Récupération des tweets de APRNEWS1\n",
            "Récupération des tweets de AfriPulse_FR\n",
            "Récupération des tweets de WhatsupicFr\n",
            "Exception get_tweets pour WhatsupicFr\n",
            "Récupération des tweets de suavelos_fr\n",
            "Exception get_tweets pour suavelos_fr\n",
            "Récupération des tweets de LBDTSS\n",
            "Récupération des tweets de le_journalnews\n",
            "Récupération des tweets de FraicheGazette\n",
            "Récupération des tweets de onaimelafrance\n",
            "Récupération des tweets de daily_express\n",
            "Avancement : 40/425\n",
            "Récupération des tweets de LeDesavantage\n",
            "Récupération des tweets de Liberaphion\n",
            "Récupération des tweets de thomasjoly60\n",
            "Récupération des tweets de EuroScoop_FR\n",
            "Récupération des tweets de drwakefield\n",
            "Récupération des tweets de allosante2016\n",
            "Exception get_tweets pour allosante2016\n",
            "Récupération des tweets de limportant_fr\n",
            "Récupération des tweets de worldtvdesinfo\n",
            "Récupération des tweets de akhnapress\n",
            "Exception get_tweets pour akhnapress\n",
            "Récupération des tweets de leouestfranc\n",
            "Avancement : 50/425\n",
            "Récupération des tweets de alertesinfos\n",
            "Exception get_tweets pour alertesinfos\n",
            "Récupération des tweets de gatewaypundit\n",
            "Exception get_tweets pour gatewaypundit\n",
            "Récupération des tweets de alainsoraloffic\n",
            "Exception get_tweets pour alainsoraloffic\n",
            "Récupération des tweets de david_vanh\n",
            "Récupération des tweets de prisonplanet\n",
            "Récupération des tweets de nosmedias_fr\n",
            "Exception get_tweets pour nosmedias_fr\n",
            "Récupération des tweets de healthranger\n",
            "Exception get_tweets pour healthranger\n",
            "Récupération des tweets de delitdimages\n",
            "Récupération des tweets de LaSireneBasque\n",
            "Exception get_tweets pour LaSireneBasque\n",
            "Récupération des tweets de OignonCantal\n",
            "Avancement : 60/425\n",
            "Récupération des tweets de wucnews\n",
            "Récupération des tweets de leweekofficiel\n",
            "Exception get_tweets pour leweekofficiel\n",
            "Récupération des tweets de tomimagfr\n",
            "Récupération des tweets de onsaitceque\n",
            "Exception get_tweets pour onsaitceque\n",
            "Récupération des tweets de BuzzArenaFr\n",
            "Récupération des tweets de actumaginfo\n",
            "Récupération des tweets de francetvdesinfo\n",
            "Récupération des tweets de AstuceDuJourMag\n",
            "Récupération des tweets de actualidadpanam\n",
            "Récupération des tweets de DailySnark\n",
            "Avancement : 70/425\n",
            "Récupération des tweets de freewoodpost\n",
            "Récupération des tweets de The_Postillon\n",
            "Récupération des tweets de thedailymash\n",
            "Récupération des tweets de akami_shimbun\n",
            "Récupération des tweets de RealAlexJones\n",
            "Exception get_tweets pour RealAlexJones\n",
            "Récupération des tweets de ravelationsmag\n",
            "Récupération des tweets de InvestigAction\n",
            "Récupération des tweets de AlertesNews\n",
            "Récupération des tweets de cyceon_fr\n",
            "Récupération des tweets de gamekult\n",
            "Avancement : 80/425\n",
            "Récupération des tweets de causeur\n",
            "Récupération des tweets de SecretNewsInfo\n",
            "Récupération des tweets de envie_devivre\n",
            "Récupération des tweets de RidiculeTV\n",
            "Récupération des tweets de Filteris\n",
            "Récupération des tweets de Europe_Israel\n",
            "Récupération des tweets de Civilwarineurop\n",
            "Récupération des tweets de spiritsciences\n",
            "Récupération des tweets de radiocockpit\n",
            "Récupération des tweets de corsemachin\n",
            "Avancement : 90/425\n",
            "Récupération des tweets de BuzzBeed\n",
            "Exception get_tweets pour BuzzBeed\n",
            "Récupération des tweets de FoxNews\n",
            "Récupération des tweets de i24NEWS_FR\n",
            "Récupération des tweets de Lesjoursfr\n",
            "Récupération des tweets de nextinpact\n",
            "Récupération des tweets de Rue89Bordeaux\n",
            "Récupération des tweets de lesoir\n",
            "Récupération des tweets de conspiration\n",
            "Récupération des tweets de Korben\n",
            "Récupération des tweets de leLab_E1\n",
            "Avancement : 100/425\n",
            "Récupération des tweets de Qofficiel\n",
            "Récupération des tweets de Bastamag\n",
            "Exception get_tweets pour Bastamag\n",
            "Récupération des tweets de lundimat1\n",
            "Récupération des tweets de Fakir_\n",
            "Récupération des tweets de PaulJorion\n",
            "Récupération des tweets de cahiersdufoot\n",
            "Récupération des tweets de acrimed_info\n",
            "Récupération des tweets de Charlie_Hebdo_\n",
            "Récupération des tweets de esantefr\n",
            "Récupération des tweets de PasseportSante\n",
            "Avancement : 110/425\n",
            "Récupération des tweets de morandinisante\n",
            "Récupération des tweets de lesucretvous\n",
            "Récupération des tweets de electroallergik\n",
            "Récupération des tweets de allodocteurs\n",
            "Récupération des tweets de AlterEco_\n",
            "Récupération des tweets de mesopinions_com\n",
            "Récupération des tweets de Change\n",
            "Récupération des tweets de buzzlyweb\n",
            "Exception get_tweets pour buzzlyweb\n",
            "Récupération des tweets de Canardpcredac\n",
            "Récupération des tweets de Confidentielles\n",
            "Avancement : 120/425\n",
            "Récupération des tweets de dirtybiology\n",
            "Récupération des tweets de epenser\n",
            "Récupération des tweets de Clubic\n",
            "Récupération des tweets de journalDfemmes\n",
            "Récupération des tweets de commentcamarche\n",
            "Récupération des tweets de JVCom\n",
            "Récupération des tweets de astucenaturelle\n",
            "Récupération des tweets de Santeplusmag\n",
            "Récupération des tweets de jpney\n",
            "Exception get_tweets pour jpney\n",
            "Récupération des tweets de humourdedroite\n",
            "Avancement : 130/425\n",
            "Récupération des tweets de maitre_eolas\n",
            "Récupération des tweets de Yagg\n",
            "Exception get_tweets pour Yagg\n",
            "Récupération des tweets de VosgesMatin\n",
            "Récupération des tweets de VICEfr\n",
            "Récupération des tweets de VICE\n",
            "Récupération des tweets de USATODAY\n",
            "Récupération des tweets de TVMAG\n",
            "Récupération des tweets de washingtonpost\n",
            "Récupération des tweets de wsj\n",
            "Récupération des tweets de nytimes\n",
            "Avancement : 140/425\n",
            "Récupération des tweets de theintercept\n",
            "Récupération des tweets de Independent\n",
            "Récupération des tweets de heraldscotland\n",
            "Récupération des tweets de guardian\n",
            "Récupération des tweets de TheEconomist\n",
            "Récupération des tweets de dailytelegraph\n",
            "Récupération des tweets de TF1\n",
            "Récupération des tweets de TETUmag\n",
            "Récupération des tweets de Telerama\n",
            "Récupération des tweets de TeleLoisirs\n",
            "Avancement : 150/425\n",
            "Récupération des tweets de Tele7\n",
            "Récupération des tweets de sudouest\n",
            "Récupération des tweets de streetpress\n",
            "Récupération des tweets de snopes\n",
            "Récupération des tweets de Slatefr\n",
            "Récupération des tweets de Sciences_Avenir\n",
            "Récupération des tweets de science_et_vie\n",
            "Récupération des tweets de Rue89\n",
            "Récupération des tweets de RTLFrance\n",
            "Récupération des tweets de RFI\n",
            "Avancement : 160/425\n",
            "Récupération des tweets de Reuters\n",
            "Récupération des tweets de Reporterre\n",
            "Récupération des tweets de Psychologies_\n",
            "Récupération des tweets de presseocean\n",
            "Récupération des tweets de PremiereFR\n",
            "Récupération des tweets de Politis_fr\n",
            "Récupération des tweets de POLITICOEurope\n",
            "Récupération des tweets de politico\n",
            "Récupération des tweets de pelerincom\n",
            "Récupération des tweets de paris_normandie\n",
            "Avancement : 170/425\n",
            "Récupération des tweets de ParisMatch\n",
            "Exception get_tweets pour ParisMatch\n",
            "Récupération des tweets de OuestFrance\n",
            "Récupération des tweets de OnzeMondial\n",
            "Récupération des tweets de Numerama\n",
            "Récupération des tweets de NotreTemps\n",
            "Récupération des tweets de NordEclairWeb\n",
            "Récupération des tweets de nonspolitique\n",
            "Récupération des tweets de Nice_Matin\n",
            "Récupération des tweets de NYDailyNews\n",
            "Récupération des tweets de mouv\n",
            "Avancement : 180/425\n",
            "Récupération des tweets de VotreArgent\n",
            "Récupération des tweets de Midilibre\n",
            "Récupération des tweets de mediapart\n",
            "Récupération des tweets de MashableFR\n",
            "Récupération des tweets de mashable\n",
            "Récupération des tweets de mariefrancemag\n",
            "Récupération des tweets de marieclaire\n",
            "Récupération des tweets de MarianneleMag\n",
            "Récupération des tweets de Madamefigaro\n",
            "Récupération des tweets de M6\n",
            "Avancement : 190/425\n",
            "Récupération des tweets de latimes\n",
            "Récupération des tweets de libe\n",
            "Récupération des tweets de lesinrocks\n",
            "Récupération des tweets de LesEchos\n",
            "Récupération des tweets de letemps\n",
            "Récupération des tweets de LeTelegramme\n",
            "Récupération des tweets de Le_Progres\n",
            "Récupération des tweets de lepopulaire_fr\n",
            "Récupération des tweets de LePoint\n",
            "Récupération des tweets de le_Parisien\n",
            "Avancement : 200/425\n",
            "Récupération des tweets de LeNouvelEco\n",
            "Récupération des tweets de mdiplo\n",
            "Récupération des tweets de lemondefr\n",
            "Récupération des tweets de lemainelibre\n",
            "Récupération des tweets de leJDD\n",
            "Récupération des tweets de lejdc_fr\n",
            "Récupération des tweets de lejsl\n",
            "Récupération des tweets de Le_Figaro\n",
            "Récupération des tweets de ledauphine\n",
            "Récupération des tweets de CourrierPicard\n",
            "Avancement : 210/425\n",
            "Récupération des tweets de LeBondyBlog\n",
            "Récupération des tweets de Lebienpublic\n",
            "Récupération des tweets de leberry_fr\n",
            "Récupération des tweets de LCP\n",
            "Récupération des tweets de LCI\n",
            "Récupération des tweets de lavoixdunord\n",
            "Récupération des tweets de LT_LaTribune\n",
            "Récupération des tweets de larep_fr\n",
            "Récupération des tweets de repubblicait\n",
            "Exception get_tweets pour repubblicait\n",
            "Récupération des tweets de laprovence\n",
            "Avancement : 220/425\n",
            "Récupération des tweets de lamontagne_fr\n",
            "Récupération des tweets de lamarsweb\n",
            "Récupération des tweets de ladepechedumidi\n",
            "Récupération des tweets de LaCroix\n",
            "Récupération des tweets de UnionArdennais\n",
            "Récupération des tweets de Lopinion_fr\n",
            "Récupération des tweets de lobs\n",
            "Récupération des tweets de linternautemag\n",
            "Récupération des tweets de lindependant\n",
            "Récupération des tweets de humanite_fr\n",
            "Avancement : 230/425\n",
            "Récupération des tweets de LEXPRESS\n",
            "Récupération des tweets de lesteclair\n",
            "Récupération des tweets de lestrepublicain\n",
            "Récupération des tweets de lequipe\n",
            "Récupération des tweets de lecho_fr\n",
            "Récupération des tweets de lecho\n",
            "Récupération des tweets de lalsace\n",
            "Récupération des tweets de cnews\n",
            "Récupération des tweets de IsFearrAnStar\n",
            "Récupération des tweets de LeHuffPost\n",
            "Avancement : 240/425\n",
            "Récupération des tweets de HoustonChron\n",
            "Récupération des tweets de hoaxbuster\n",
            "Récupération des tweets de hoaxslayer\n",
            "Récupération des tweets de historiamag\n",
            "Récupération des tweets de Grazia\n",
            "Récupération des tweets de glamourmag\n",
            "Récupération des tweets de franceinfo\n",
            "Récupération des tweets de franceinter\n",
            "Récupération des tweets de franceculture\n",
            "Récupération des tweets de francebleu\n",
            "Avancement : 250/425\n",
            "Récupération des tweets de France5tv\n",
            "Récupération des tweets de France3tv\n",
            "Récupération des tweets de FRANCE24\n",
            "Récupération des tweets de France2tv\n",
            "Récupération des tweets de FinancialTimes\n",
            "Récupération des tweets de femmeactuelle\n",
            "Récupération des tweets de Europe1\n",
            "Récupération des tweets de euronews\n",
            "Récupération des tweets de cnewsmatin\n",
            "Exception get_tweets pour cnewsmatin\n",
            "Récupération des tweets de derStandardat\n",
            "Avancement : 260/425\n",
            "Récupération des tweets de DerSPIEGEL\n",
            "Récupération des tweets de denverpost\n",
            "Récupération des tweets de DebunkerHED\n",
            "Récupération des tweets de courrierinter\n",
            "Récupération des tweets de Cosmopolitan\n",
            "Récupération des tweets de Corse_Matin\n",
            "Récupération des tweets de Corriere\n",
            "Récupération des tweets de CNN\n",
            "Récupération des tweets de chicagotribune\n",
            "Récupération des tweets de charentelibre\n",
            "Avancement : 270/425\n",
            "Récupération des tweets de Challenges\n",
            "Récupération des tweets de MagazineCapital\n",
            "Récupération des tweets de canalplus\n",
            "Récupération des tweets de BuzzFeed\n",
            "Récupération des tweets de businessinsider\n",
            "Récupération des tweets de Boursorama\n",
            "Récupération des tweets de Bloomberg\n",
            "Récupération des tweets de bibamagazine\n",
            "Récupération des tweets de BFMTV\n",
            "Récupération des tweets de BBC\n",
            "Avancement : 280/425\n",
            "Récupération des tweets de atlantico_fr\n",
            "Récupération des tweets de AP\n",
            "Récupération des tweets de ARTEfr\n",
            "Récupération des tweets de arretsurimages\n",
            "Récupération des tweets de ameli_actu\n",
            "Exception get_tweets pour ameli_actu\n",
            "Récupération des tweets de AlterNet\n",
            "Récupération des tweets de ajplus\n",
            "Récupération des tweets de AJENews\n",
            "Récupération des tweets de afpfr\n",
            "Récupération des tweets de ABC\n",
            "Avancement : 290/425\n",
            "Récupération des tweets de 20Minutes\n",
            "Récupération des tweets de Valeurs\n",
            "Récupération des tweets de thedailybeast\n",
            "Récupération des tweets de morandiniblog\n",
            "Récupération des tweets de Gentside\n",
            "Récupération des tweets de ohmymagfr\n",
            "Récupération des tweets de leseconoclastes\n",
            "Récupération des tweets de tvlofficiel\n",
            "Récupération des tweets de topito_com\n",
            "Récupération des tweets de other98\n",
            "Avancement : 300/425\n",
            "Récupération des tweets de UnleashMind\n",
            "Récupération des tweets de TariqRamadan\n",
            "Récupération des tweets de sputnik_fr\n",
            "Récupération des tweets de spi0n\n",
            "Récupération des tweets de santeici\n",
            "Exception get_tweets pour santeici\n",
            "Récupération des tweets de RTenfrancais\n",
            "Récupération des tweets de RT_com\n",
            "Exception get_tweets pour RT_com\n",
            "Récupération des tweets de ruptly\n",
            "Récupération des tweets de PublicFR\n",
            "Exception get_tweets pour PublicFR\n",
            "Récupération des tweets de ThePOSITIVR\n",
            "Avancement : 310/425\n",
            "Récupération des tweets de PauseCafein\n",
            "Récupération des tweets de oumma\n",
            "Récupération des tweets de OnlineMagazin\n",
            "Récupération des tweets de ojim_france\n",
            "Récupération des tweets de occupydemocrats\n",
            "Récupération des tweets de novopress\n",
            "Exception get_tweets pour novopress\n",
            "Récupération des tweets de ndffr\n",
            "Récupération des tweets de nypost\n",
            "Récupération des tweets de Mondi_Alisation\n",
            "Récupération des tweets de minutebuzz\n",
            "Avancement : 320/425\n",
            "Récupération des tweets de MetaTVFrance\n",
            "Récupération des tweets de melty_fr\n",
            "Récupération des tweets de LesNews\n",
            "Récupération des tweets de Letribunaldunet\n",
            "Récupération des tweets de JournalduSiecle\n",
            "Exception get_tweets pour JournalduSiecle\n",
            "Récupération des tweets de zapeuzefolle\n",
            "Exception get_tweets pour zapeuzefolle\n",
            "Récupération des tweets de KonbiniFr\n",
            "Récupération des tweets de JSSNews\n",
            "Récupération des tweets de actu140\n",
            "Récupération des tweets de HumourDeMec\n",
            "Avancement : 330/425\n",
            "Récupération des tweets de Hitekfr\n",
            "Récupération des tweets de HeraultTribune\n",
            "Récupération des tweets de F_Desouche\n",
            "Récupération des tweets de Etienne_Chouard\n",
            "Récupération des tweets de Enquete_etDebat\n",
            "Récupération des tweets de DRUDGE_REPORT\n",
            "Exception get_tweets pour DRUDGE_REPORT\n",
            "Récupération des tweets de doctissimo\n",
            "Récupération des tweets de Demotivateur\n",
            "Récupération des tweets de MailOnline\n",
            "Récupération des tweets de NatCounterPunch\n",
            "Avancement : 340/425\n",
            "Récupération des tweets de contribuables\n",
            "Récupération des tweets de Contrepoints\n",
            "Récupération des tweets de BuzzFilcom\n",
            "Récupération des tweets de BreitbartNews\n",
            "Récupération des tweets de BVoltaire\n",
            "Récupération des tweets de bipartisanism\n",
            "Récupération des tweets de beforeitsnews\n",
            "Exception get_tweets pour beforeitsnews\n",
            "Récupération des tweets de americannewsx\n",
            "Récupération des tweets de Alkanz\n",
            "Récupération des tweets de AgenceInfoLibre\n",
            "Avancement : 350/425\n",
            "Récupération des tweets de InfoMdia\n",
            "Récupération des tweets de Dreuz_1fo\n",
            "Récupération des tweets de infolibnews\n",
            "Récupération des tweets de YannMerkado\n",
            "Exception get_tweets pour YannMerkado\n",
            "Récupération des tweets de WikistrikeW\n",
            "Récupération des tweets de WantedPedo\n",
            "Récupération des tweets de tprincedelamour\n",
            "Récupération des tweets de SwagActu\n",
            "Récupération des tweets de RussiaInsider\n",
            "Récupération des tweets de 1RiposteLaique\n",
            "Avancement : 360/425\n",
            "Récupération des tweets de reseau_internat\n",
            "Récupération des tweets de ReinformationRC\n",
            "Récupération des tweets de RedState\n",
            "Récupération des tweets de paris_vox\n",
            "Récupération des tweets de Panamza\n",
            "Récupération des tweets de nord_actu\n",
            "Récupération des tweets de NiceProvenceInf\n",
            "Récupération des tweets de ObservateursCH\n",
            "Récupération des tweets de oberruyer\n",
            "Récupération des tweets de PrJoyeux\n",
            "Avancement : 370/425\n",
            "Récupération des tweets de LaGaucheMaTuer\n",
            "Exception get_tweets pour LaGaucheMaTuer\n",
            "Récupération des tweets de linsoumis_fr\n",
            "Exception get_tweets pour linsoumis_fr\n",
            "Récupération des tweets de linformatrice\n",
            "Exception get_tweets pour linformatrice\n",
            "Récupération des tweets de pierrejovanovic\n",
            "Récupération des tweets de JeuneNation\n",
            "Récupération des tweets de infosbordeaux\n",
            "Récupération des tweets de infowars\n",
            "Exception get_tweets pour infowars\n",
            "Récupération des tweets de infovsinfo\n",
            "Récupération des tweets de David_vanH\n",
            "Récupération des tweets de degage_hollande\n",
            "Exception get_tweets pour degage_hollande\n",
            "Avancement : 380/425\n",
            "Récupération des tweets de Herissident\n",
            "Exception get_tweets pour Herissident\n",
            "Récupération des tweets de HenryMakow\n",
            "Exception get_tweets pour HenryMakow\n",
            "Récupération des tweets de guy_fawkes_news\n",
            "Récupération des tweets de EetR_National\n",
            "Récupération des tweets de diktacratie\n",
            "Récupération des tweets de MbalaDieudo\n",
            "Récupération des tweets de davidicke\n",
            "Exception get_tweets pour davidicke\n",
            "Récupération des tweets de DailyNewsBin\n",
            "Récupération des tweets de corbettreport\n",
            "Récupération des tweets de breizh_info\n",
            "Avancement : 390/425\n",
            "Récupération des tweets de boris_lay\n",
            "Exception get_tweets pour boris_lay\n",
            "Récupération des tweets de barenakedislam\n",
            "Récupération des tweets de antipresse_net\n",
            "Récupération des tweets de AE911Truth\n",
            "Récupération des tweets de 21WIRE\n",
            "Exception get_tweets pour 21WIRE\n",
            "Récupération des tweets de LNParadigme\n",
            "Exception get_tweets pour LNParadigme\n",
            "Récupération des tweets de theonion\n",
            "Récupération des tweets de nordpresse\n",
            "Exception get_tweets pour nordpresse\n",
            "Récupération des tweets de newsthump\n",
            "Récupération des tweets de Le_Navet\n",
            "Avancement : 400/425\n",
            "Récupération des tweets de LeJurafi\n",
            "Récupération des tweets de le_gorafi\n",
            "Récupération des tweets de sardinedp\n",
            "Récupération des tweets de iepique\n",
            "Récupération des tweets de echodelaboucle\n",
            "Récupération des tweets de thehuzlers\n",
            "Exception get_tweets pour thehuzlers\n",
            "Récupération des tweets de FootballFrance_\n",
            "Récupération des tweets de el_manchar\n",
            "Récupération des tweets de edukactus\n",
            "Récupération des tweets de clickhole\n",
            "Avancement : 410/425\n",
            "Récupération des tweets de bravepatrie\n",
            "Récupération des tweets de Branchednews\n",
            "Récupération des tweets de Boulevard69\n",
            "Récupération des tweets de boucherieovalie\n",
            "Récupération des tweets de BilboquetMag\n",
            "Récupération des tweets de amplifyingG\n",
            "Récupération des tweets de 24matin\n",
            "Exception get_tweets pour 24matin\n",
            "Récupération des tweets de Yahoo\n",
            "Récupération des tweets de twitter.com\n",
            "Exception get_tweets pour twitter.com\n",
            "Récupération des tweets de LePlus\n",
            "Avancement : 420/425\n",
            "Récupération des tweets de FigaroVox\n",
            "Récupération des tweets de agoravox\n",
            "Récupération des tweets de 9GAG\n",
            "Récupération des tweets de Wikipedia\n",
            "Récupération des tweets de wikiHow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vérification"
      ],
      "metadata": {
        "id": "t3a_iAS8FtdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "xlq1_oKGHClu",
        "outputId": "57f14f64-7561-40c2-cb58-5da3c7857976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         user_id user_screen_name             id_tweet          created_at  \\\n",
              "0       78957336      france_soir  1507008907390062602 2022-03-24 14:58:17   \n",
              "1       78957336      france_soir  1506984886061699072 2022-03-24 13:22:49   \n",
              "2       78957336      france_soir  1506959719319093249 2022-03-24 11:42:49   \n",
              "3       78957336      france_soir  1506958411115675650 2022-03-24 11:37:37   \n",
              "4       78957336      france_soir  1506953912259121156 2022-03-24 11:19:45   \n",
              "...          ...              ...                  ...                 ...   \n",
              "997975  14354304          wikiHow   925845110461534209 2017-11-01 22:00:50   \n",
              "997976  14354304          wikiHow   925843158973366272 2017-11-01 21:53:05   \n",
              "997977  14354304          wikiHow   925814991772676096 2017-11-01 20:01:09   \n",
              "997978  14354304          wikiHow   925784921842634753 2017-11-01 18:01:40   \n",
              "997979  14354304          wikiHow   925754713856978944 2017-11-01 16:01:38   \n",
              "\n",
              "                                                    tweet  retweet_count  \\\n",
              "0       🎙️ \"Un Corse indépendantiste, c’est à la base ...              3   \n",
              "1       🍄  La start-up Mycophyto propose d’enrichir le...              8   \n",
              "2       📽 Dans un debriefing exclusif conduit avec Ari...             54   \n",
              "3       ⚠️ Le conflit #UkraineRussie créant à la fois ...             21   \n",
              "4       🖍️ Le dessin du jour sur #McKinseyGate, par Z_...            213   \n",
              "...                                                   ...            ...   \n",
              "997975  A little bit of gentle heat from a hair dryer ...              0   \n",
              "997976  @pxkelley @hiringourheroes Go out there and ge...              0   \n",
              "997977  #wikitips https://t.co/58AmUzhsaj https://t.co...              0   \n",
              "997978  If your dishwasher still holds water when you ...              1   \n",
              "997979  Usually, when dealing with venomous spiders, v...              0   \n",
              "\n",
              "        favorite_count  liability     liability_label  count_followers  \n",
              "0                   11          2     Site non fiable           126112  \n",
              "1                   31          2     Site non fiable           126112  \n",
              "2                   85          2     Site non fiable           126112  \n",
              "3                   36          2     Site non fiable           126112  \n",
              "4                  350          2     Site non fiable           126112  \n",
              "...                ...        ...                 ...              ...  \n",
              "997975               4          4  Site réputé fiable            73816  \n",
              "997976               0          4  Site réputé fiable            73816  \n",
              "997977               2          4  Site réputé fiable            73816  \n",
              "997978               3          4  Site réputé fiable            73816  \n",
              "997979               2          4  Site réputé fiable            73816  \n",
              "\n",
              "[997980 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8ce76c5-4b57-454d-9906-7c21e693c193\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_screen_name</th>\n",
              "      <th>id_tweet</th>\n",
              "      <th>created_at</th>\n",
              "      <th>tweet</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>liability</th>\n",
              "      <th>liability_label</th>\n",
              "      <th>count_followers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>78957336</td>\n",
              "      <td>france_soir</td>\n",
              "      <td>1507008907390062602</td>\n",
              "      <td>2022-03-24 14:58:17</td>\n",
              "      <td>🎙️ \"Un Corse indépendantiste, c’est à la base ...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>Site non fiable</td>\n",
              "      <td>126112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>78957336</td>\n",
              "      <td>france_soir</td>\n",
              "      <td>1506984886061699072</td>\n",
              "      <td>2022-03-24 13:22:49</td>\n",
              "      <td>🍄  La start-up Mycophyto propose d’enrichir le...</td>\n",
              "      <td>8</td>\n",
              "      <td>31</td>\n",
              "      <td>2</td>\n",
              "      <td>Site non fiable</td>\n",
              "      <td>126112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>78957336</td>\n",
              "      <td>france_soir</td>\n",
              "      <td>1506959719319093249</td>\n",
              "      <td>2022-03-24 11:42:49</td>\n",
              "      <td>📽 Dans un debriefing exclusif conduit avec Ari...</td>\n",
              "      <td>54</td>\n",
              "      <td>85</td>\n",
              "      <td>2</td>\n",
              "      <td>Site non fiable</td>\n",
              "      <td>126112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>78957336</td>\n",
              "      <td>france_soir</td>\n",
              "      <td>1506958411115675650</td>\n",
              "      <td>2022-03-24 11:37:37</td>\n",
              "      <td>⚠️ Le conflit #UkraineRussie créant à la fois ...</td>\n",
              "      <td>21</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>Site non fiable</td>\n",
              "      <td>126112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>78957336</td>\n",
              "      <td>france_soir</td>\n",
              "      <td>1506953912259121156</td>\n",
              "      <td>2022-03-24 11:19:45</td>\n",
              "      <td>🖍️ Le dessin du jour sur #McKinseyGate, par Z_...</td>\n",
              "      <td>213</td>\n",
              "      <td>350</td>\n",
              "      <td>2</td>\n",
              "      <td>Site non fiable</td>\n",
              "      <td>126112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997975</th>\n",
              "      <td>14354304</td>\n",
              "      <td>wikiHow</td>\n",
              "      <td>925845110461534209</td>\n",
              "      <td>2017-11-01 22:00:50</td>\n",
              "      <td>A little bit of gentle heat from a hair dryer ...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Site réputé fiable</td>\n",
              "      <td>73816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997976</th>\n",
              "      <td>14354304</td>\n",
              "      <td>wikiHow</td>\n",
              "      <td>925843158973366272</td>\n",
              "      <td>2017-11-01 21:53:05</td>\n",
              "      <td>@pxkelley @hiringourheroes Go out there and ge...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>Site réputé fiable</td>\n",
              "      <td>73816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997977</th>\n",
              "      <td>14354304</td>\n",
              "      <td>wikiHow</td>\n",
              "      <td>925814991772676096</td>\n",
              "      <td>2017-11-01 20:01:09</td>\n",
              "      <td>#wikitips https://t.co/58AmUzhsaj https://t.co...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>Site réputé fiable</td>\n",
              "      <td>73816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997978</th>\n",
              "      <td>14354304</td>\n",
              "      <td>wikiHow</td>\n",
              "      <td>925784921842634753</td>\n",
              "      <td>2017-11-01 18:01:40</td>\n",
              "      <td>If your dishwasher still holds water when you ...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>Site réputé fiable</td>\n",
              "      <td>73816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997979</th>\n",
              "      <td>14354304</td>\n",
              "      <td>wikiHow</td>\n",
              "      <td>925754713856978944</td>\n",
              "      <td>2017-11-01 16:01:38</td>\n",
              "      <td>Usually, when dealing with venomous spiders, v...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>Site réputé fiable</td>\n",
              "      <td>73816</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>997980 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8ce76c5-4b57-454d-9906-7c21e693c193')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8ce76c5-4b57-454d-9906-7c21e693c193 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8ce76c5-4b57-454d-9906-7c21e693c193');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sauvegarde des tweets"
      ],
      "metadata": {
        "id": "v2Fp0buMFrIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res.to_pickle(DATA_PATH+\"data_tweets.pkl\")"
      ],
      "metadata": {
        "id": "3vnI0nDe58be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Section inutile étant donné la résolution du problème qu'elle traitait\n",
        "\n",
        "Auparavant suite a certaines erreurs la récupération des tweets des 425 utilisateurs retenus saturait la RAM. Une autre solution pour récupérer les tweets a depuis été implémenté et, maintenant, il n'y a plus de problème de saturation et donc plus la nécessité d'utiliser cette partie du programme."
      ],
      "metadata": {
        "id": "ZavsPsGqE5fZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAoDfaEzGvqE"
      },
      "source": [
        "Création d'une classe TwitterUser contenant les fonctions de récupération de données (utilisateurs, tweets, ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1VhX0_ENen"
      },
      "source": [
        "\"\"\"class TwitterUser():\n",
        "  _screen_name = \"\"\n",
        "  _id = \"\"\n",
        "  _user_data = None\n",
        "  followers = []\n",
        "  following = []\n",
        "  tweet = []\n",
        "  like = []\n",
        "  request_count = 0\n",
        "\n",
        "  def __init__(self, sname = \"\", _json = {}):\n",
        "    if (sname != \"\"):\n",
        "      self._screen_name = sname\n",
        "      self.get_info()\n",
        "      if (_json != {}):\n",
        "        self._json = _json\n",
        "\n",
        "  def get_info(self):\n",
        "    try:\n",
        "      self._user_data = api.get_user(screen_name=self._screen_name)\n",
        "    except Exception as e:\n",
        "      print(\"Exception get_info\")\n",
        "\n",
        "  def get_all_data(self):\n",
        "    self.get_tweets()\n",
        "    # self.get_followers()\n",
        "    # self.get_following()\n",
        "    # self.get_like()\n",
        "\n",
        "  def set_name(self, name):\n",
        "    self._screen_name = name\n",
        "\n",
        "  def set_id(self, _id):\n",
        "    self._id = _id\n",
        "\n",
        "  def get_followers(self):\n",
        "    with tqdm(total=self._user_data.followers_count, desc=\"get_followers for \" + self._user_data.screen_name) as pbar:\n",
        "      for page in tweepy.Cursor(api.followers, screen_name=self._screen_name, count=200).pages():\n",
        "        self.followers.extend(page)\n",
        "        pbar.update(len(page))\n",
        "\n",
        "\n",
        "  def get_following(self):\n",
        "    with tqdm(total=self._user_data.friends_count, desc=\"get_following for \" + self._user_data.screen_name) as pbar:\n",
        "      for page in tweepy.Cursor(api.friends, screen_name=self._screen_name, count=200).pages():\n",
        "        self.following.extend(page)\n",
        "        pbar.update(len(page))\n",
        "\n",
        "  # on ne peut prendre que les 1000 derniers\n",
        "  def get_tweets(self):\n",
        "    try:\n",
        "      with tqdm(total=1000, desc=\"get_tweets for \" + self._user_data.screen_name) as pbar:\n",
        "        for page in tweepy.Cursor(api.user_timeline, screen_name=self._screen_name, count=200).pages():\n",
        "          self.tweet.extend(page)\n",
        "          pbar.update(len(page))\n",
        "    except Exception as e:\n",
        "      print(\"Exception get_tweets\")\n",
        "\n",
        "  # on ne peut prendre que les 3000 derniers\n",
        "  def get_like(self):\n",
        "    if self._user_data.protected:\n",
        "      print(\"can get like from \" + self._user_data.screen_name + \", the account is protected\")\n",
        "    else:\n",
        "      with tqdm(total=3000, desc=\"get_likes for \" + self._user_data.screen_name) as pbar:\n",
        "        for page in tweepy.Cursor(api.favorites, screen_name=self._screen_name, count=200).pages():\n",
        "          self.like.extend(page)\n",
        "          pbar.update(len(page))\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Récupération des tweets"
      ],
      "metadata": {
        "id": "L0dBF1p9FnBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"list_tweet = []\n",
        "\n",
        "for site in data:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "\n",
        "for tweet in user.tweet:\n",
        "  list_tweet = list_tweet + [tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]]\"\"\""
      ],
      "metadata": {
        "id": "lboMLEbnFmhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-U_RpJEp5by"
      },
      "source": [
        "Division de la liste des site en plusieurs sous-listes pour ne pas exéder la RAM alloué par Google Colab "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPHt_5sDp32s"
      },
      "source": [
        "# def split_list(a_list):\n",
        "#     half = len(a_list)//2\n",
        "#     return a_list[:half], a_list[half:]\n",
        "\n",
        "# dataA, dataB = split_list(data)\n",
        "# data1, data2 = split_list(dataA)\n",
        "# data3, data4 = split_list(dataB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5sjAkR7PwlP"
      },
      "source": [
        "Autre manière de séparer en plusieurs sous-listes : séparation basée sur le niveau de fiabilité des sites."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acQU7jpTOS-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "05d5a24d-8fa0-4849-f61b-3dd5d57fd1e2"
      },
      "source": [
        "\"\"\"data1 = [site for site in data if site[1] == 1]\n",
        "data2 = [site for site in data if site[1] == 2]\n",
        "data3 = [site for site in data if site[1] == 3]\n",
        "data4 = [site for site in data if site[1] == 4]\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data1 = [site for site in data if site[1] == 1]\\ndata2 = [site for site in data if site[1] == 2]\\ndata3 = [site for site in data if site[1] == 3]\\ndata4 = [site for site in data if site[1] == 4]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfLfTerE4v8w"
      },
      "source": [
        "Ajout de tous les tweets dans 4 tableaux qui vont contenir les données importantes de chaque tweet. Chaque tableau contient un quart des données de ***data***. Les tableaux sont ensuite sauvegardé au format pkl sur le Drive\n",
        "\n",
        "Pour obtenir chaque morceau il faut relancer la session (ce qui libère la RAM) et exécuter le bloc de commande contenant le morceau voulu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxG51-Y8Qh_e"
      },
      "source": [
        "Création et sauvegarde du tableau 1\n",
        "\n",
        "Tentative de supprimer le problème de sur-copie des tweets (tweets du dernier utilisateur apparaissent 1 fois, tweets de l'avant dernier 2 fois, encore avant 3 fois, ...)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"list_tweet = []\n",
        "\n",
        "for site in data1:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "  for tweet in user.tweet:\n",
        "    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\n",
        "\n",
        "res = pd.DataFrame(list_tweet, columns=['user_id', 'id_tweet', 'tweet', 'retweet_count', 'favorite_count', 'label_liability'])\n",
        "res.to_pickle(DATA_PATH+\"quartier1.pkl\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "A6RXI8McD7Mg",
        "outputId": "3dc56354-b6b8-4c68-fd92-52e7409b836c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'list_tweet = []\\n\\nfor site in data1:\\n  user = TwitterUser(sname= site[2])\\n  user.get_all_data()\\n  for tweet in user.tweet:\\n    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\\n\\nres = pd.DataFrame(list_tweet, columns=[\\'user_id\\', \\'id_tweet\\', \\'tweet\\', \\'retweet_count\\', \\'favorite_count\\', \\'label_liability\\'])\\nres.to_pickle(DATA_PATH+\"quartier1.pkl\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"len(list_tweet)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BWZFRKFvxL_M",
        "outputId": "dffe5b1c-63f7-4844-9816-89f86c6c58bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'len(list_tweet)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsm2agoEQosG"
      },
      "source": [
        "Création et sauvegarde du tableau 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLHc-ZS9CP0E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "99400d80-14f0-4a67-cccd-14e51894d4ac"
      },
      "source": [
        "\"\"\"list_tweet = []\n",
        "\n",
        "for site in data2:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "  for tweet in user.tweet:\n",
        "    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\n",
        "\n",
        "res = pd.DataFrame(list_tweet, columns=['user_id', 'id_tweet', 'tweet', 'retweet_count', 'favorite_count', 'label_liability'])\n",
        "res.to_pickle(DATA_PATH+\"quartier2.pkl\")\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'list_tweet = []\\n\\nfor site in data2:\\n  user = TwitterUser(sname= site[2])\\n  user.get_all_data()\\n  for tweet in user.tweet:\\n    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\\n\\nres = pd.DataFrame(list_tweet, columns=[\\'user_id\\', \\'id_tweet\\', \\'tweet\\', \\'retweet_count\\', \\'favorite_count\\', \\'label_liability\\'])\\nres.to_pickle(DATA_PATH+\"quartier2.pkl\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUMIiSmOQphw"
      },
      "source": [
        "Création et sauvegarde du tableau 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-IGxUhfQsfI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "54dabece-d51e-45de-9c5d-3d8c1bd655c0"
      },
      "source": [
        "\"\"\"list_tweet = []\n",
        "\n",
        "for site in data2:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "  for tweet in user.tweet:\n",
        "    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\n",
        "\n",
        "res = pd.DataFrame(list_tweet, columns=['user_id', 'id_tweet', 'tweet', 'retweet_count', 'favorite_count', 'label_liability'])\n",
        "res.to_pickle(DATA_PATH+\"quartier3.pkl\")\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'list_tweet = []\\n\\nfor site in data2:\\n  user = TwitterUser(sname= site[2])\\n  user.get_all_data()\\n  for tweet in user.tweet:\\n    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\\n\\nres = pd.DataFrame(list_tweet, columns=[\\'user_id\\', \\'id_tweet\\', \\'tweet\\', \\'retweet_count\\', \\'favorite_count\\', \\'label_liability\\'])\\nres.to_pickle(DATA_PATH+\"quartier3.pkl\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmPLcpIKQqdb"
      },
      "source": [
        "Création et sauvegarde du tableau 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV5rrGYlQt1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "175866f2-0904-49b5-9be4-affebfb39dc8"
      },
      "source": [
        "\"\"\"list_tweet = []\n",
        "\n",
        "for site in data2:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "  for tweet in user.tweet:\n",
        "    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\n",
        "\n",
        "res = pd.DataFrame(list_tweet, columns=['user_id', 'id_tweet', 'tweet', 'retweet_count', 'favorite_count', 'label_liability'])\n",
        "res.to_pickle(DATA_PATH+\"quartier4.pkl\")\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'list_tweet = []\\n\\nfor site in data2:\\n  user = TwitterUser(sname= site[2])\\n  user.get_all_data()\\n  for tweet in user.tweet:\\n    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\\n\\nres = pd.DataFrame(list_tweet, columns=[\\'user_id\\', \\'id_tweet\\', \\'tweet\\', \\'retweet_count\\', \\'favorite_count\\', \\'label_liability\\'])\\nres.to_pickle(DATA_PATH+\"quartier4.pkl\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jV_qISFQvcC"
      },
      "source": [
        "Après avoir obtenu les 4 tableaux en ayant relancé 4 sessions, une pour la création de chaque tableau, on récupère les 4 tableaux"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsAnHYjgQRWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bbff1f37-42d4-4779-cac0-6b3eb951732e"
      },
      "source": [
        "\"\"\"tableauTweets1 = pd.read_pickle(DATA_PATH+\"quartier1.pkl\")\n",
        "tableauTweets2 = pd.read_pickle(DATA_PATH+\"quartier2.pkl\")\n",
        "tableauTweets3 = pd.read_pickle(DATA_PATH+\"quartier3.pkl\")\n",
        "tableauTweets4 = pd.read_pickle(DATA_PATH+\"quartier4.pkl\")\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tableauTweets1 = pd.read_pickle(DATA_PATH+\"quartier1.pkl\")\\ntableauTweets2 = pd.read_pickle(DATA_PATH+\"quartier2.pkl\")\\ntableauTweets3 = pd.read_pickle(DATA_PATH+\"quartier3.pkl\")\\ntableauTweets4 = pd.read_pickle(DATA_PATH+\"quartier4.pkl\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIY_aadkqc58"
      },
      "source": [
        "Concaténation des 4 tableaux en un seul et sauvegarde dans un seul fichier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkkYRw4Urer0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "872604f7-1f82-44d4-bfef-2679fd654724"
      },
      "source": [
        "\"\"\"tableauTweets = pd.concat([tableauTweets1,tableauTweets2,tableauTweets3,tableauTweets4])\n",
        "tableauTweets.reset_index(drop=True)\n",
        "tableauTweets.to_pickle(DATA_PATH+\"tableauTweets.pkl\")\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tableauTweets = pd.concat([tableauTweets1,tableauTweets2,tableauTweets3,tableauTweets4])\\ntableauTweets.reset_index(drop=True)\\ntableauTweets.to_pickle(DATA_PATH+\"tableauTweets.pkl\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvhGQXIHq8wC"
      },
      "source": [
        "Réinitialisation du fichier PauchBas.pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YGrwhVLp-XM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c8ab313a-13f8-439f-9cce-f66d25776b3b"
      },
      "source": [
        "\"\"\"import pickle\n",
        "\n",
        "with open(DATA_PATH+\"PaucBas.pickle\", \"wb\") as obj_file:\n",
        "    pickle.dump([], obj_file, -1)\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import pickle\\n\\nwith open(DATA_PATH+\"PaucBas.pickle\", \"wb\") as obj_file:\\n    pickle.dump([], obj_file, -1)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhSJo6vwG8kn"
      },
      "source": [
        "Sauvegarde des données reçu pour chaque utilisateur Twitter recherché"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J7FWlgrrFTx"
      },
      "source": [
        "# with open(DATA_PATH+\"PaucBas.pickle\", \"ab\") as obj_file:\n",
        "#   for site in data1:\n",
        "#     user = TwitterUser(sname= site[2])\n",
        "#     user.get_all_data()\n",
        "#     pickle.dump([user], obj_file, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uqFx3lwrFEU"
      },
      "source": [
        "# with open(DATA_PATH+\"PaucBas.pickle\", \"ab\") as obj_file:\n",
        "#   for site in data2:\n",
        "#     user = TwitterUser(sname= site[2])\n",
        "#     user.get_all_data()\n",
        "#     pickle.dump([user], obj_file, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}